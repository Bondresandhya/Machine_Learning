{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67c944b",
   "metadata": {},
   "source": [
    "###### Implement Reinforcement Learning using an example of a maze environment that the agent needs to explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd738df-b89d-44a4-ba5e-b9581b033f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Q-Table:\n",
      "[[[4.07858705e+00 4.78296900e+00 4.12258226e+00 4.07336268e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [1.47628670e-02 1.12204559e+00 6.47648353e-02 9.12077368e-03]\n",
      "  [8.01244413e-03 9.13238703e-04 2.04338346e-02 9.97503302e-02]\n",
      "  [4.44005319e-02 6.14097001e-01 6.11335922e-03 7.64159670e-04]]\n",
      "\n",
      " [[3.96732717e+00 5.31441000e+00 4.72653931e+00 4.64079392e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [2.20530095e-01 6.46795008e+00 1.56147528e+00 3.01543114e-01]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [3.66263416e-02 4.44735293e+00 8.62721343e-01 1.98886102e-01]]\n",
      "\n",
      " [[4.67717358e+00 4.37400806e+00 5.26271269e+00 5.90490000e+00]\n",
      "  [5.82284244e+00 5.77781717e+00 5.22906484e+00 6.56100000e+00]\n",
      "  [5.46187359e+00 7.29000000e+00 5.76939139e+00 6.39872949e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [1.52648120e+00 8.98162765e+00 2.75481715e+00 3.48321285e+00]]\n",
      "\n",
      " [[5.22452006e+00 3.28643341e-01 7.28907987e-01 1.11709392e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [6.51132228e+00 6.16362658e+00 7.26351467e+00 8.10000000e+00]\n",
      "  [8.08049521e+00 8.07451721e+00 7.27409336e+00 9.00000000e+00]\n",
      "  [7.97409081e+00 1.00000000e+01 7.99815880e+00 8.93649941e+00]]\n",
      "\n",
      " [[1.62581503e+00 6.76672761e-02 1.82171430e-03 2.15198355e-01]\n",
      "  [3.72704870e-02 6.14095276e-02 4.96003721e-03 2.28530384e+00]\n",
      "  [7.21274666e+00 2.25423077e+00 3.81347695e-01 1.47719532e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the maze environment\n",
    "maze = [\n",
    "    [0, -1, 0, 0, 0],\n",
    "    [0, -1, 0, -1, 0],\n",
    "    [0, 0, 0, -1, 0],\n",
    "    [0, -1, 0, 0, 0],\n",
    "    [0, 0, 0, -1, 10]  # Goal is the cell with a reward of +10\n",
    "]\n",
    "\n",
    "# Parameters\n",
    "num_rows, num_cols = 5, 5  # Maze dimensions\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "epsilon = 0.9  # Exploration rate\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "num_episodes = 1000  # Number of episodes\n",
    "\n",
    "# Initialize Q-table\n",
    "Q_table = np.zeros((num_rows, num_cols, len(actions)))\n",
    "\n",
    "# Helper functions\n",
    "def is_valid_move(x, y):\n",
    "    return 0 <= x < num_rows and 0 <= y < num_cols and maze[x][y] != -1\n",
    "\n",
    "def get_next_state(x, y, action):\n",
    "    if action == 'up':\n",
    "        return (x - 1, y) if is_valid_move(x - 1, y) else (x, y)\n",
    "    elif action == 'down':\n",
    "        return (x + 1, y) if is_valid_move(x + 1, y) else (x, y)\n",
    "    elif action == 'left':\n",
    "        return (x, y - 1) if is_valid_move(x, y - 1) else (x, y)\n",
    "    elif action == 'right':\n",
    "        return (x, y + 1) if is_valid_move(x, y + 1) else (x, y)\n",
    "    return x, y\n",
    "\n",
    "# Training the agent\n",
    "for episode in range(num_episodes):\n",
    "    x, y = 0, 0  # Starting position\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Choose an action (with epsilon-greedy strategy)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action_index = random.randint(0, len(actions) - 1)  # Explore\n",
    "        else:\n",
    "            action_index = np.argmax(Q_table[x, y])  # Exploit\n",
    "        \n",
    "        action = actions[action_index]\n",
    "        \n",
    "        # Take the action and observe the reward\n",
    "        next_x, next_y = get_next_state(x, y, action)\n",
    "        reward = maze[next_x][next_y]\n",
    "        \n",
    "        # Update Q-table\n",
    "        old_value = Q_table[x, y, action_index]\n",
    "        next_max = np.max(Q_table[next_x, next_y])\n",
    "        \n",
    "        # Q-learning formula\n",
    "        Q_table[x, y, action_index] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "        \n",
    "        # Update state\n",
    "        x, y = next_x, next_y\n",
    "        \n",
    "        # Check if the goal is reached\n",
    "        if reward == 10:\n",
    "            done = True\n",
    "\n",
    "    # Decay epsilon over time\n",
    "    epsilon = max(0.1, epsilon * 0.99)\n",
    "\n",
    "print(\"Training complete! Q-Table:\")\n",
    "print(Q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb110d-3d63-4c44-b342-81751600fe17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
